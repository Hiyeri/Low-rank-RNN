{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision making in low-rank recurrent neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x22b3695a490>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#plt.style.use('dark_background')\n",
    "\n",
    "# For debugging we will use the below inside some functions.\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptual decision makingg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Create a function for generating the data.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining standard deviation and strength of stimulus\n",
    "strength = 0.032\n",
    "stim_strength = strength * np.hstack((-np.flip(2**np.arange(0,5)), 2**np.arange(0,5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check the dimensions needed for the target y.\n",
    "\n",
    "\n",
    "def generate_data(time_length, trials, stim_strength, std = 0.03):\n",
    "    \"\"\"\n",
    "    ksi is a normally distr noise with sigma 0.03.\n",
    "    u is an input\n",
    "    û is the stimulis strength\n",
    "    y is the target, i.e. the sign of û. \n",
    "    y's zeroth dimension is trials. \n",
    "    \"\"\"\n",
    "    ksi = std * np.random.randn(time_length, trials)\n",
    "    \n",
    "  \n",
    "    û_value = np.random.choice(stim_strength, size=trials)\n",
    "    û = np.full_like(ksi, 0)\n",
    "    y = np.full_like(trials, 1)\n",
    "    for i in range(trials):  # vectorize it??\n",
    "        û[5:46, i] = û_value[i]\n",
    "        y = np.sign(û_value)\n",
    "    \n",
    "    return  torch.from_numpy(û + ksi).reshape([time_length, trials, 1, 1]), torch.from_numpy(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "\n",
    "T = 75\n",
    "trials = 8\n",
    "u, y = generate_data(T, trials, stim_strength)\n",
    "time = np.arange(0,T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1., dtype=torch.float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.shape, y.shape\n",
    "y[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x144 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "\n",
    "fig, axs = plt.subplots(1, trials, figsize=(15,2))\n",
    "\n",
    "fig.suptitle(\"Generate Data\")\n",
    "\n",
    "for i in range(trials):\n",
    "    axs[i].plot(time, u[:, i, 0, 0], label=\"stimulus\")\n",
    "    axs[i].plot(time, y[i]*np.full_like(time,1), label=\"target\")\n",
    "    axs[i].set_title(f\"trial {i}\")\n",
    "    axs[i].set_ylim(-1.2, 1.2)\n",
    "axs[0].set_xlabel(\"time\")\n",
    "axs[0].set_ylabel(\"signal strength\")\n",
    "axs[0].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Implement the recurrent neural network in your favorite deep learning library.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J matrix has to have two trainable vectors m, and n as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: vectorize input and targets and all into batch of time and trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo: finish refactoring for batch dim, reduce the reshapings.\n",
    "\n",
    "def dynamics(x, u_of_t, tau, m,n,I):\n",
    "    J_of_phi_x = torch.div(torch.matmul(torch.matmul(m, n).float(), torch.tanh(x).float()), len(n))\n",
    "\n",
    "    return (-x + J_of_phi_x + u_of_t*I) / tau  # is broadcasting working for input and in weigths?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 124, 1]), torch.Size([2, 124, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test dynamics.\n",
    "x = torch.randn([2,124,1])   # trials, size of the network, and 1 dimension\n",
    "I = torch.randn([124,1])   # size of the network and 1 dimension\n",
    "\n",
    "u, y = generate_data(74, 2, stim_strength) # time, trials, stimulis strength\n",
    "tau = 5\n",
    "\n",
    "\n",
    "m = torch.randn(2, 124, 1)\n",
    "n = torch.randn(2, 1, 124)\n",
    "(u[50]*I).shape, dynamics(x, u[50], tau, m, n, I).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LR_RNN(nn.Module):\n",
    "    # Low-rank recurrent neural network without readout.\n",
    "    def __init__(self, N):\n",
    "        super().__init__()\n",
    "        \n",
    "        # storign info\n",
    "        self.N = N\n",
    "        \n",
    "        # pattern from witch connectivity is made\n",
    "        tensor_m = torch.randn([N,1])\n",
    "        self.m = nn.Parameter(tensor_m)\n",
    "        \n",
    "        # pattern to which connectivity is made\n",
    "        tensor_n = torch.randn([1,N])\n",
    "        self.n = nn.Parameter(tensor_n)\n",
    "        \n",
    "        # input weights\n",
    "        self.I = torch.randn([N,1])\n",
    "        \n",
    "        # Output is defined separetly in different class below.\n",
    "\n",
    "          \n",
    "    def forward(self, x, u_of_t, tau, dt):\n",
    "        # It's an Euler method.\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        \n",
    "        return x +  dt * dynamics(x, u_of_t, tau, self.m, self.n, self.I)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio is  20.0 ms/step, while tau in steps is  5.0  and time increment in dynamics is  1.0\n"
     ]
    }
   ],
   "source": [
    "ratio = 1500/75 #ms/step\n",
    "tau_steps = 100 /ratio #ms/(ms/step)\n",
    "dt = 20 /ratio\n",
    "\n",
    "print(\"Ratio is \", ratio, \"ms/step, while tau in steps is \", tau_steps, \" and time increment in dynamics is \", dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, trials, N):\n",
    "        super().__init__()\n",
    "        # storing info\n",
    "        self.N = N\n",
    "        \n",
    "        # the layer\n",
    "        self.lr_rnn = LR_RNN(N)\n",
    "        \n",
    "        # output layer\n",
    "        std_dev = 4\n",
    "        self.w = std_dev*torch.randn([1, N])\n",
    "        \n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, u_of_t, tau, dt):\n",
    "        \n",
    "        \"\"\"\n",
    "        Going forward with the low rank rnn and input.\n",
    "        Reading through electrode weights.\n",
    "        \"\"\"\n",
    "        x_next = self.lr_rnn(x, u_of_t, tau, dt)\n",
    "\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        \n",
    "        z = 1/self.N * torch.matmul(self.w.float(), torch.tanh(x_next).float())\n",
    "        z.squeeze_()\n",
    "        return x_next, z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check dimensions for multiplication self.w with tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize.\n",
    "\n",
    "N = 128\n",
    "trials = 32    # Mini-batch size, minimum value is 2 (probably because of squeezing and broadcasting).\n",
    "model = Model(trials, N)\n",
    "x = torch.zeros([trials,N,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio is  20.0 ms/step, while tau in steps is  5.0  and time increment in dynamics is  1.0\n"
     ]
    }
   ],
   "source": [
    "ratio = 1500/75 #ms/step\n",
    "tau = 100 /ratio #ms/(ms/step)\n",
    "dt = 20 /ratio\n",
    "\n",
    "print(\"Ratio is \", ratio, \"ms/step, while tau in steps is \", tau, \" and time increment in dynamics is \", dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data.\n",
    "\n",
    "T = 75\n",
    "read_onset = 15  # how many last steps are read\n",
    "time = np.arange(0,T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train general network (generate data new for each epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Szaibot\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:446: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([15, 32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 / 1000 epoch mean error is  ------------   tensor(0.6756, grad_fn=<MseLossBackward>)\n",
      "   1 / 1000 epoch mean error is  ------------   tensor(0.7023, grad_fn=<MseLossBackward>)\n",
      "   2 / 1000 epoch mean error is  ------------   tensor(0.6058, grad_fn=<MseLossBackward>)\n",
      "   3 / 1000 epoch mean error is  ------------   tensor(0.7387, grad_fn=<MseLossBackward>)\n",
      "   4 / 1000 epoch mean error is  ------------   tensor(1.0284, grad_fn=<MseLossBackward>)\n",
      "   5 / 1000 epoch mean error is  ------------   tensor(1.1255, grad_fn=<MseLossBackward>)\n",
      "   6 / 1000 epoch mean error is  ------------   tensor(0.8559, grad_fn=<MseLossBackward>)\n",
      "   7 / 1000 epoch mean error is  ------------   tensor(0.7633, grad_fn=<MseLossBackward>)\n",
      "   8 / 1000 epoch mean error is  ------------   tensor(0.9030, grad_fn=<MseLossBackward>)\n",
      "   9 / 1000 epoch mean error is  ------------   tensor(1.1426, grad_fn=<MseLossBackward>)\n",
      "   10 / 1000 epoch mean error is  ------------   tensor(0.7614, grad_fn=<MseLossBackward>)\n",
      "   11 / 1000 epoch mean error is  ------------   tensor(0.7613, grad_fn=<MseLossBackward>)\n",
      "   12 / 1000 epoch mean error is  ------------   tensor(1.1927, grad_fn=<MseLossBackward>)\n",
      "   13 / 1000 epoch mean error is  ------------   tensor(0.9043, grad_fn=<MseLossBackward>)\n",
      "   14 / 1000 epoch mean error is  ------------   tensor(1.2810, grad_fn=<MseLossBackward>)\n",
      "   15 / 1000 epoch mean error is  ------------   tensor(1.0882, grad_fn=<MseLossBackward>)\n",
      "   16 / 1000 epoch mean error is  ------------   tensor(0.9028, grad_fn=<MseLossBackward>)\n",
      "   17 / 1000 epoch mean error is  ------------   tensor(1.0344, grad_fn=<MseLossBackward>)\n",
      "   18 / 1000 epoch mean error is  ------------   tensor(0.7723, grad_fn=<MseLossBackward>)\n",
      "   19 / 1000 epoch mean error is  ------------   tensor(0.8169, grad_fn=<MseLossBackward>)\n",
      "   20 / 1000 epoch mean error is  ------------   tensor(0.9426, grad_fn=<MseLossBackward>)\n",
      "   21 / 1000 epoch mean error is  ------------   tensor(1.0236, grad_fn=<MseLossBackward>)\n",
      "   22 / 1000 epoch mean error is  ------------   tensor(0.8206, grad_fn=<MseLossBackward>)\n",
      "   23 / 1000 epoch mean error is  ------------   tensor(0.7030, grad_fn=<MseLossBackward>)\n",
      "   24 / 1000 epoch mean error is  ------------   tensor(1.0971, grad_fn=<MseLossBackward>)\n",
      "   25 / 1000 epoch mean error is  ------------   tensor(1.0560, grad_fn=<MseLossBackward>)\n",
      "   26 / 1000 epoch mean error is  ------------   tensor(0.9393, grad_fn=<MseLossBackward>)\n",
      "   27 / 1000 epoch mean error is  ------------   tensor(1.1260, grad_fn=<MseLossBackward>)\n",
      "   28 / 1000 epoch mean error is  ------------   tensor(0.9752, grad_fn=<MseLossBackward>)\n",
      "   29 / 1000 epoch mean error is  ------------   tensor(0.9018, grad_fn=<MseLossBackward>)\n",
      "   30 / 1000 epoch mean error is  ------------   tensor(1.0436, grad_fn=<MseLossBackward>)\n",
      "   31 / 1000 epoch mean error is  ------------   tensor(0.8684, grad_fn=<MseLossBackward>)\n",
      "   32 / 1000 epoch mean error is  ------------   tensor(0.8354, grad_fn=<MseLossBackward>)\n",
      "   33 / 1000 epoch mean error is  ------------   tensor(0.9705, grad_fn=<MseLossBackward>)\n",
      "   34 / 1000 epoch mean error is  ------------   tensor(0.8710, grad_fn=<MseLossBackward>)\n",
      "   35 / 1000 epoch mean error is  ------------   tensor(1.0349, grad_fn=<MseLossBackward>)\n",
      "   36 / 1000 epoch mean error is  ------------   tensor(1.0013, grad_fn=<MseLossBackward>)\n",
      "   37 / 1000 epoch mean error is  ------------   tensor(0.9687, grad_fn=<MseLossBackward>)\n",
      "   38 / 1000 epoch mean error is  ------------   tensor(0.9994, grad_fn=<MseLossBackward>)\n",
      "   39 / 1000 epoch mean error is  ------------   tensor(0.8761, grad_fn=<MseLossBackward>)\n",
      "   40 / 1000 epoch mean error is  ------------   tensor(0.9072, grad_fn=<MseLossBackward>)\n",
      "   41 / 1000 epoch mean error is  ------------   tensor(0.9968, grad_fn=<MseLossBackward>)\n",
      "   42 / 1000 epoch mean error is  ------------   tensor(0.9372, grad_fn=<MseLossBackward>)\n",
      "   43 / 1000 epoch mean error is  ------------   tensor(1.1116, grad_fn=<MseLossBackward>)\n",
      "   44 / 1000 epoch mean error is  ------------   tensor(0.9092, grad_fn=<MseLossBackward>)\n",
      "   45 / 1000 epoch mean error is  ------------   tensor(0.9938, grad_fn=<MseLossBackward>)\n",
      "   46 / 1000 epoch mean error is  ------------   tensor(1.0204, grad_fn=<MseLossBackward>)\n",
      "   47 / 1000 epoch mean error is  ------------   tensor(0.9921, grad_fn=<MseLossBackward>)\n",
      "   48 / 1000 epoch mean error is  ------------   tensor(1.0176, grad_fn=<MseLossBackward>)\n",
      "   49 / 1000 epoch mean error is  ------------   tensor(0.9648, grad_fn=<MseLossBackward>)\n",
      "   50 / 1000 epoch mean error is  ------------   tensor(0.9896, grad_fn=<MseLossBackward>)\n",
      "   51 / 1000 epoch mean error is  ------------   tensor(0.9646, grad_fn=<MseLossBackward>)\n",
      "   52 / 1000 epoch mean error is  ------------   tensor(0.8221, grad_fn=<MseLossBackward>)\n",
      "   53 / 1000 epoch mean error is  ------------   tensor(1.0584, grad_fn=<MseLossBackward>)\n",
      "   54 / 1000 epoch mean error is  ------------   tensor(0.8952, grad_fn=<MseLossBackward>)\n",
      "   55 / 1000 epoch mean error is  ------------   tensor(0.8957, grad_fn=<MseLossBackward>)\n",
      "   56 / 1000 epoch mean error is  ------------   tensor(0.8959, grad_fn=<MseLossBackward>)\n",
      "   57 / 1000 epoch mean error is  ------------   tensor(0.7342, grad_fn=<MseLossBackward>)\n",
      "   58 / 1000 epoch mean error is  ------------   tensor(1.0114, grad_fn=<MseLossBackward>)\n",
      "   59 / 1000 epoch mean error is  ------------   tensor(0.9880, grad_fn=<MseLossBackward>)\n",
      "   60 / 1000 epoch mean error is  ------------   tensor(0.9394, grad_fn=<MseLossBackward>)\n",
      "   61 / 1000 epoch mean error is  ------------   tensor(0.9393, grad_fn=<MseLossBackward>)\n",
      "   62 / 1000 epoch mean error is  ------------   tensor(0.9388, grad_fn=<MseLossBackward>)\n",
      "   63 / 1000 epoch mean error is  ------------   tensor(1.0151, grad_fn=<MseLossBackward>)\n",
      "   64 / 1000 epoch mean error is  ------------   tensor(0.7577, grad_fn=<MseLossBackward>)\n",
      "   65 / 1000 epoch mean error is  ------------   tensor(0.9901, grad_fn=<MseLossBackward>)\n",
      "   66 / 1000 epoch mean error is  ------------   tensor(1.0173, grad_fn=<MseLossBackward>)\n",
      "   67 / 1000 epoch mean error is  ------------   tensor(0.9363, grad_fn=<MseLossBackward>)\n",
      "   68 / 1000 epoch mean error is  ------------   tensor(0.9913, grad_fn=<MseLossBackward>)\n",
      "   69 / 1000 epoch mean error is  ------------   tensor(0.8810, grad_fn=<MseLossBackward>)\n",
      "   70 / 1000 epoch mean error is  ------------   tensor(0.9644, grad_fn=<MseLossBackward>)\n",
      "   71 / 1000 epoch mean error is  ------------   tensor(1.0207, grad_fn=<MseLossBackward>)\n",
      "   72 / 1000 epoch mean error is  ------------   tensor(0.9359, grad_fn=<MseLossBackward>)\n",
      "   73 / 1000 epoch mean error is  ------------   tensor(0.8789, grad_fn=<MseLossBackward>)\n",
      "   74 / 1000 epoch mean error is  ------------   tensor(0.9637, grad_fn=<MseLossBackward>)\n",
      "   75 / 1000 epoch mean error is  ------------   tensor(0.9932, grad_fn=<MseLossBackward>)\n",
      "   76 / 1000 epoch mean error is  ------------   tensor(0.8210, grad_fn=<MseLossBackward>)\n",
      "   77 / 1000 epoch mean error is  ------------   tensor(0.7889, grad_fn=<MseLossBackward>)\n",
      "   78 / 1000 epoch mean error is  ------------   tensor(1.0535, grad_fn=<MseLossBackward>)\n",
      "   79 / 1000 epoch mean error is  ------------   tensor(0.9638, grad_fn=<MseLossBackward>)\n",
      "   80 / 1000 epoch mean error is  ------------   tensor(0.8738, grad_fn=<MseLossBackward>)\n",
      "   81 / 1000 epoch mean error is  ------------   tensor(0.9639, grad_fn=<MseLossBackward>)\n",
      "   82 / 1000 epoch mean error is  ------------   tensor(0.8419, grad_fn=<MseLossBackward>)\n",
      "   83 / 1000 epoch mean error is  ------------   tensor(0.9971, grad_fn=<MseLossBackward>)\n",
      "   84 / 1000 epoch mean error is  ------------   tensor(0.8399, grad_fn=<MseLossBackward>)\n",
      "   85 / 1000 epoch mean error is  ------------   tensor(1.0631, grad_fn=<MseLossBackward>)\n",
      "   86 / 1000 epoch mean error is  ------------   tensor(0.9011, grad_fn=<MseLossBackward>)\n",
      "   87 / 1000 epoch mean error is  ------------   tensor(0.8046, grad_fn=<MseLossBackward>)\n",
      "   88 / 1000 epoch mean error is  ------------   tensor(0.9666, grad_fn=<MseLossBackward>)\n",
      "   89 / 1000 epoch mean error is  ------------   tensor(0.9338, grad_fn=<MseLossBackward>)\n",
      "   90 / 1000 epoch mean error is  ------------   tensor(0.7684, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   91 / 1000 epoch mean error is  ------------   tensor(0.7994, grad_fn=<MseLossBackward>)\n",
      "   92 / 1000 epoch mean error is  ------------   tensor(1.0035, grad_fn=<MseLossBackward>)\n",
      "   93 / 1000 epoch mean error is  ------------   tensor(0.8286, grad_fn=<MseLossBackward>)\n",
      "   94 / 1000 epoch mean error is  ------------   tensor(1.0446, grad_fn=<MseLossBackward>)\n",
      "   95 / 1000 epoch mean error is  ------------   tensor(0.8985, grad_fn=<MseLossBackward>)\n",
      "   96 / 1000 epoch mean error is  ------------   tensor(0.9360, grad_fn=<MseLossBackward>)\n",
      "   97 / 1000 epoch mean error is  ------------   tensor(1.0456, grad_fn=<MseLossBackward>)\n",
      "   98 / 1000 epoch mean error is  ------------   tensor(0.9708, grad_fn=<MseLossBackward>)\n",
      "   99 / 1000 epoch mean error is  ------------   tensor(0.8625, grad_fn=<MseLossBackward>)\n",
      "   100 / 1000 epoch mean error is  ------------   tensor(1.0049, grad_fn=<MseLossBackward>)\n",
      "   101 / 1000 epoch mean error is  ------------   tensor(0.9676, grad_fn=<MseLossBackward>)\n",
      "   102 / 1000 epoch mean error is  ------------   tensor(1.1687, grad_fn=<MseLossBackward>)\n",
      "   103 / 1000 epoch mean error is  ------------   tensor(0.8035, grad_fn=<MseLossBackward>)\n",
      "   104 / 1000 epoch mean error is  ------------   tensor(0.9623, grad_fn=<MseLossBackward>)\n",
      "   105 / 1000 epoch mean error is  ------------   tensor(0.9624, grad_fn=<MseLossBackward>)\n",
      "   106 / 1000 epoch mean error is  ------------   tensor(0.9331, grad_fn=<MseLossBackward>)\n",
      "   107 / 1000 epoch mean error is  ------------   tensor(0.8504, grad_fn=<MseLossBackward>)\n",
      "   108 / 1000 epoch mean error is  ------------   tensor(0.7670, grad_fn=<MseLossBackward>)\n",
      "   109 / 1000 epoch mean error is  ------------   tensor(0.8784, grad_fn=<MseLossBackward>)\n",
      "   110 / 1000 epoch mean error is  ------------   tensor(1.0499, grad_fn=<MseLossBackward>)\n",
      "   111 / 1000 epoch mean error is  ------------   tensor(0.8765, grad_fn=<MseLossBackward>)\n",
      "   112 / 1000 epoch mean error is  ------------   tensor(0.8749, grad_fn=<MseLossBackward>)\n",
      "   113 / 1000 epoch mean error is  ------------   tensor(0.7857, grad_fn=<MseLossBackward>)\n",
      "   114 / 1000 epoch mean error is  ------------   tensor(0.8098, grad_fn=<MseLossBackward>)\n",
      "   115 / 1000 epoch mean error is  ------------   tensor(0.8330, grad_fn=<MseLossBackward>)\n",
      "   116 / 1000 epoch mean error is  ------------   tensor(0.9308, grad_fn=<MseLossBackward>)\n",
      "   117 / 1000 epoch mean error is  ------------   tensor(0.8972, grad_fn=<MseLossBackward>)\n",
      "   118 / 1000 epoch mean error is  ------------   tensor(0.8972, grad_fn=<MseLossBackward>)\n",
      "   119 / 1000 epoch mean error is  ------------   tensor(1.0135, grad_fn=<MseLossBackward>)\n",
      "   120 / 1000 epoch mean error is  ------------   tensor(1.1331, grad_fn=<MseLossBackward>)\n",
      "   121 / 1000 epoch mean error is  ------------   tensor(0.9749, grad_fn=<MseLossBackward>)\n",
      "   122 / 1000 epoch mean error is  ------------   tensor(0.7429, grad_fn=<MseLossBackward>)\n",
      "   123 / 1000 epoch mean error is  ------------   tensor(0.8981, grad_fn=<MseLossBackward>)\n",
      "   124 / 1000 epoch mean error is  ------------   tensor(1.0135, grad_fn=<MseLossBackward>)\n",
      "   125 / 1000 epoch mean error is  ------------   tensor(1.0129, grad_fn=<MseLossBackward>)\n",
      "   126 / 1000 epoch mean error is  ------------   tensor(0.8591, grad_fn=<MseLossBackward>)\n",
      "   127 / 1000 epoch mean error is  ------------   tensor(1.0097, grad_fn=<MseLossBackward>)\n",
      "   128 / 1000 epoch mean error is  ------------   tensor(1.1917, grad_fn=<MseLossBackward>)\n",
      "   129 / 1000 epoch mean error is  ------------   tensor(0.8607, grad_fn=<MseLossBackward>)\n",
      "   130 / 1000 epoch mean error is  ------------   tensor(0.8643, grad_fn=<MseLossBackward>)\n",
      "   131 / 1000 epoch mean error is  ------------   tensor(0.9953, grad_fn=<MseLossBackward>)\n",
      "   132 / 1000 epoch mean error is  ------------   tensor(0.7773, grad_fn=<MseLossBackward>)\n",
      "   133 / 1000 epoch mean error is  ------------   tensor(0.8106, grad_fn=<MseLossBackward>)\n",
      "   134 / 1000 epoch mean error is  ------------   tensor(0.8697, grad_fn=<MseLossBackward>)\n",
      "   135 / 1000 epoch mean error is  ------------   tensor(0.9617, grad_fn=<MseLossBackward>)\n",
      "   136 / 1000 epoch mean error is  ------------   tensor(0.8692, grad_fn=<MseLossBackward>)\n",
      "   137 / 1000 epoch mean error is  ------------   tensor(0.9298, grad_fn=<MseLossBackward>)\n",
      "   138 / 1000 epoch mean error is  ------------   tensor(0.8988, grad_fn=<MseLossBackward>)\n",
      "   139 / 1000 epoch mean error is  ------------   tensor(0.9929, grad_fn=<MseLossBackward>)\n",
      "   140 / 1000 epoch mean error is  ------------   tensor(0.9961, grad_fn=<MseLossBackward>)\n",
      "   141 / 1000 epoch mean error is  ------------   tensor(0.9285, grad_fn=<MseLossBackward>)\n",
      "   142 / 1000 epoch mean error is  ------------   tensor(0.8668, grad_fn=<MseLossBackward>)\n",
      "   143 / 1000 epoch mean error is  ------------   tensor(0.7724, grad_fn=<MseLossBackward>)\n",
      "   144 / 1000 epoch mean error is  ------------   tensor(0.9283, grad_fn=<MseLossBackward>)\n",
      "   145 / 1000 epoch mean error is  ------------   tensor(1.0270, grad_fn=<MseLossBackward>)\n",
      "   146 / 1000 epoch mean error is  ------------   tensor(0.9597, grad_fn=<MseLossBackward>)\n",
      "   147 / 1000 epoch mean error is  ------------   tensor(0.9291, grad_fn=<MseLossBackward>)\n",
      "   148 / 1000 epoch mean error is  ------------   tensor(1.0564, grad_fn=<MseLossBackward>)\n",
      "   149 / 1000 epoch mean error is  ------------   tensor(0.9274, grad_fn=<MseLossBackward>)\n",
      "   150 / 1000 epoch mean error is  ------------   tensor(0.8664, grad_fn=<MseLossBackward>)\n",
      "   151 / 1000 epoch mean error is  ------------   tensor(0.9264, grad_fn=<MseLossBackward>)\n",
      "   152 / 1000 epoch mean error is  ------------   tensor(0.8095, grad_fn=<MseLossBackward>)\n",
      "   153 / 1000 epoch mean error is  ------------   tensor(0.9300, grad_fn=<MseLossBackward>)\n",
      "   154 / 1000 epoch mean error is  ------------   tensor(0.8375, grad_fn=<MseLossBackward>)\n",
      "   155 / 1000 epoch mean error is  ------------   tensor(1.0218, grad_fn=<MseLossBackward>)\n",
      "   156 / 1000 epoch mean error is  ------------   tensor(0.8603, grad_fn=<MseLossBackward>)\n",
      "   157 / 1000 epoch mean error is  ------------   tensor(0.6672, grad_fn=<MseLossBackward>)\n",
      "   158 / 1000 epoch mean error is  ------------   tensor(1.0271, grad_fn=<MseLossBackward>)\n",
      "   159 / 1000 epoch mean error is  ------------   tensor(0.8546, grad_fn=<MseLossBackward>)\n",
      "   160 / 1000 epoch mean error is  ------------   tensor(0.9638, grad_fn=<MseLossBackward>)\n",
      "   161 / 1000 epoch mean error is  ------------   tensor(0.8136, grad_fn=<MseLossBackward>)\n",
      "   162 / 1000 epoch mean error is  ------------   tensor(1.1993, grad_fn=<MseLossBackward>)\n",
      "   163 / 1000 epoch mean error is  ------------   tensor(1.0344, grad_fn=<MseLossBackward>)\n",
      "   164 / 1000 epoch mean error is  ------------   tensor(0.8170, grad_fn=<MseLossBackward>)\n",
      "   165 / 1000 epoch mean error is  ------------   tensor(0.7147, grad_fn=<MseLossBackward>)\n",
      "   166 / 1000 epoch mean error is  ------------   tensor(0.9504, grad_fn=<MseLossBackward>)\n",
      "   167 / 1000 epoch mean error is  ------------   tensor(0.7232, grad_fn=<MseLossBackward>)\n",
      "   168 / 1000 epoch mean error is  ------------   tensor(0.7822, grad_fn=<MseLossBackward>)\n",
      "   169 / 1000 epoch mean error is  ------------   tensor(0.9540, grad_fn=<MseLossBackward>)\n",
      "   170 / 1000 epoch mean error is  ------------   tensor(0.8100, grad_fn=<MseLossBackward>)\n",
      "   171 / 1000 epoch mean error is  ------------   tensor(0.8844, grad_fn=<MseLossBackward>)\n",
      "   172 / 1000 epoch mean error is  ------------   tensor(0.6503, grad_fn=<MseLossBackward>)\n",
      "   173 / 1000 epoch mean error is  ------------   tensor(0.9725, grad_fn=<MseLossBackward>)\n",
      "   174 / 1000 epoch mean error is  ------------   tensor(0.7253, grad_fn=<MseLossBackward>)\n",
      "   175 / 1000 epoch mean error is  ------------   tensor(0.9762, grad_fn=<MseLossBackward>)\n",
      "   176 / 1000 epoch mean error is  ------------   tensor(0.7147, grad_fn=<MseLossBackward>)\n",
      "   177 / 1000 epoch mean error is  ------------   tensor(0.8962, grad_fn=<MseLossBackward>)\n",
      "   178 / 1000 epoch mean error is  ------------   tensor(0.7522, grad_fn=<MseLossBackward>)\n",
      "   179 / 1000 epoch mean error is  ------------   tensor(0.7531, grad_fn=<MseLossBackward>)\n",
      "   180 / 1000 epoch mean error is  ------------   tensor(0.6568, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   181 / 1000 epoch mean error is  ------------   tensor(0.8450, grad_fn=<MseLossBackward>)\n",
      "   182 / 1000 epoch mean error is  ------------   tensor(0.7028, grad_fn=<MseLossBackward>)\n",
      "   183 / 1000 epoch mean error is  ------------   tensor(0.6491, grad_fn=<MseLossBackward>)\n",
      "   184 / 1000 epoch mean error is  ------------   tensor(0.6948, grad_fn=<MseLossBackward>)\n",
      "   185 / 1000 epoch mean error is  ------------   tensor(1.0514, grad_fn=<MseLossBackward>)\n",
      "   186 / 1000 epoch mean error is  ------------   tensor(0.7452, grad_fn=<MseLossBackward>)\n",
      "   187 / 1000 epoch mean error is  ------------   tensor(0.8473, grad_fn=<MseLossBackward>)\n",
      "   188 / 1000 epoch mean error is  ------------   tensor(0.8933, grad_fn=<MseLossBackward>)\n",
      "   189 / 1000 epoch mean error is  ------------   tensor(0.8715, grad_fn=<MseLossBackward>)\n",
      "   190 / 1000 epoch mean error is  ------------   tensor(0.8080, grad_fn=<MseLossBackward>)\n",
      "   191 / 1000 epoch mean error is  ------------   tensor(0.8594, grad_fn=<MseLossBackward>)\n",
      "   192 / 1000 epoch mean error is  ------------   tensor(0.9325, grad_fn=<MseLossBackward>)\n",
      "   193 / 1000 epoch mean error is  ------------   tensor(0.8287, grad_fn=<MseLossBackward>)\n",
      "   194 / 1000 epoch mean error is  ------------   tensor(0.8825, grad_fn=<MseLossBackward>)\n",
      "   195 / 1000 epoch mean error is  ------------   tensor(0.7776, grad_fn=<MseLossBackward>)\n",
      "   196 / 1000 epoch mean error is  ------------   tensor(0.7041, grad_fn=<MseLossBackward>)\n",
      "   197 / 1000 epoch mean error is  ------------   tensor(0.9202, grad_fn=<MseLossBackward>)\n",
      "   198 / 1000 epoch mean error is  ------------   tensor(0.8051, grad_fn=<MseLossBackward>)\n",
      "   199 / 1000 epoch mean error is  ------------   tensor(0.7950, grad_fn=<MseLossBackward>)\n",
      "   200 / 1000 epoch mean error is  ------------   tensor(0.8660, grad_fn=<MseLossBackward>)\n",
      "   201 / 1000 epoch mean error is  ------------   tensor(0.6951, grad_fn=<MseLossBackward>)\n",
      "   202 / 1000 epoch mean error is  ------------   tensor(0.6200, grad_fn=<MseLossBackward>)\n",
      "   203 / 1000 epoch mean error is  ------------   tensor(0.7167, grad_fn=<MseLossBackward>)\n",
      "   204 / 1000 epoch mean error is  ------------   tensor(0.7241, grad_fn=<MseLossBackward>)\n",
      "   205 / 1000 epoch mean error is  ------------   tensor(0.5322, grad_fn=<MseLossBackward>)\n",
      "   206 / 1000 epoch mean error is  ------------   tensor(0.5086, grad_fn=<MseLossBackward>)\n",
      "   207 / 1000 epoch mean error is  ------------   tensor(0.4853, grad_fn=<MseLossBackward>)\n",
      "   208 / 1000 epoch mean error is  ------------   tensor(0.3830, grad_fn=<MseLossBackward>)\n",
      "   209 / 1000 epoch mean error is  ------------   tensor(0.5231, grad_fn=<MseLossBackward>)\n",
      "   210 / 1000 epoch mean error is  ------------   tensor(0.4295, grad_fn=<MseLossBackward>)\n",
      "   211 / 1000 epoch mean error is  ------------   tensor(0.3151, grad_fn=<MseLossBackward>)\n",
      "   212 / 1000 epoch mean error is  ------------   tensor(0.2372, grad_fn=<MseLossBackward>)\n",
      "   213 / 1000 epoch mean error is  ------------   tensor(0.2309, grad_fn=<MseLossBackward>)\n",
      "   214 / 1000 epoch mean error is  ------------   tensor(0.2637, grad_fn=<MseLossBackward>)\n",
      "   215 / 1000 epoch mean error is  ------------   tensor(0.2291, grad_fn=<MseLossBackward>)\n",
      "   216 / 1000 epoch mean error is  ------------   tensor(0.2063, grad_fn=<MseLossBackward>)\n",
      "   217 / 1000 epoch mean error is  ------------   tensor(0.2433, grad_fn=<MseLossBackward>)\n",
      "   218 / 1000 epoch mean error is  ------------   tensor(0.2112, grad_fn=<MseLossBackward>)\n",
      "   219 / 1000 epoch mean error is  ------------   tensor(0.2063, grad_fn=<MseLossBackward>)\n",
      "   220 / 1000 epoch mean error is  ------------   tensor(0.2096, grad_fn=<MseLossBackward>)\n",
      "   221 / 1000 epoch mean error is  ------------   tensor(0.1662, grad_fn=<MseLossBackward>)\n",
      "   222 / 1000 epoch mean error is  ------------   tensor(0.1100, grad_fn=<MseLossBackward>)\n",
      "   223 / 1000 epoch mean error is  ------------   tensor(0.1051, grad_fn=<MseLossBackward>)\n",
      "   224 / 1000 epoch mean error is  ------------   tensor(0.1395, grad_fn=<MseLossBackward>)\n",
      "   225 / 1000 epoch mean error is  ------------   tensor(0.1036, grad_fn=<MseLossBackward>)\n",
      "   226 / 1000 epoch mean error is  ------------   tensor(0.1486, grad_fn=<MseLossBackward>)\n",
      "   227 / 1000 epoch mean error is  ------------   tensor(0.1015, grad_fn=<MseLossBackward>)\n",
      "   228 / 1000 epoch mean error is  ------------   tensor(0.1013, grad_fn=<MseLossBackward>)\n",
      "   229 / 1000 epoch mean error is  ------------   tensor(0.1196, grad_fn=<MseLossBackward>)\n",
      "   230 / 1000 epoch mean error is  ------------   tensor(0.1007, grad_fn=<MseLossBackward>)\n",
      "   231 / 1000 epoch mean error is  ------------   tensor(0.1005, grad_fn=<MseLossBackward>)\n",
      "   232 / 1000 epoch mean error is  ------------   tensor(0.1206, grad_fn=<MseLossBackward>)\n",
      "   233 / 1000 epoch mean error is  ------------   tensor(0.1004, grad_fn=<MseLossBackward>)\n",
      "   234 / 1000 epoch mean error is  ------------   tensor(0.1195, grad_fn=<MseLossBackward>)\n",
      "   235 / 1000 epoch mean error is  ------------   tensor(0.1003, grad_fn=<MseLossBackward>)\n",
      "   236 / 1000 epoch mean error is  ------------   tensor(0.1009, grad_fn=<MseLossBackward>)\n",
      "   237 / 1000 epoch mean error is  ------------   tensor(0.1187, grad_fn=<MseLossBackward>)\n",
      "   238 / 1000 epoch mean error is  ------------   tensor(0.0996, grad_fn=<MseLossBackward>)\n",
      "   239 / 1000 epoch mean error is  ------------   tensor(0.1388, grad_fn=<MseLossBackward>)\n",
      "   240 / 1000 epoch mean error is  ------------   tensor(0.1758, grad_fn=<MseLossBackward>)\n",
      "   241 / 1000 epoch mean error is  ------------   tensor(0.1168, grad_fn=<MseLossBackward>)\n",
      "   242 / 1000 epoch mean error is  ------------   tensor(0.0994, grad_fn=<MseLossBackward>)\n",
      "   243 / 1000 epoch mean error is  ------------   tensor(0.1408, grad_fn=<MseLossBackward>)\n",
      "   244 / 1000 epoch mean error is  ------------   tensor(0.1338, grad_fn=<MseLossBackward>)\n",
      "   245 / 1000 epoch mean error is  ------------   tensor(0.0992, grad_fn=<MseLossBackward>)\n",
      "   246 / 1000 epoch mean error is  ------------   tensor(0.0997, grad_fn=<MseLossBackward>)\n",
      "   247 / 1000 epoch mean error is  ------------   tensor(0.1824, grad_fn=<MseLossBackward>)\n",
      "   248 / 1000 epoch mean error is  ------------   tensor(0.2204, grad_fn=<MseLossBackward>)\n",
      "   249 / 1000 epoch mean error is  ------------   tensor(0.1578, grad_fn=<MseLossBackward>)\n",
      "   250 / 1000 epoch mean error is  ------------   tensor(0.1352, grad_fn=<MseLossBackward>)\n",
      "   251 / 1000 epoch mean error is  ------------   tensor(0.1174, grad_fn=<MseLossBackward>)\n",
      "   252 / 1000 epoch mean error is  ------------   tensor(0.1567, grad_fn=<MseLossBackward>)\n",
      "   253 / 1000 epoch mean error is  ------------   tensor(0.1556, grad_fn=<MseLossBackward>)\n",
      "   254 / 1000 epoch mean error is  ------------   tensor(0.1747, grad_fn=<MseLossBackward>)\n",
      "   255 / 1000 epoch mean error is  ------------   tensor(0.1569, grad_fn=<MseLossBackward>)\n",
      "   256 / 1000 epoch mean error is  ------------   tensor(0.1218, grad_fn=<MseLossBackward>)\n",
      "   257 / 1000 epoch mean error is  ------------   tensor(0.0989, grad_fn=<MseLossBackward>)\n",
      "   258 / 1000 epoch mean error is  ------------   tensor(0.0991, grad_fn=<MseLossBackward>)\n",
      "   259 / 1000 epoch mean error is  ------------   tensor(0.1403, grad_fn=<MseLossBackward>)\n",
      "   260 / 1000 epoch mean error is  ------------   tensor(0.1977, grad_fn=<MseLossBackward>)\n",
      "   261 / 1000 epoch mean error is  ------------   tensor(0.2218, grad_fn=<MseLossBackward>)\n",
      "   262 / 1000 epoch mean error is  ------------   tensor(0.1389, grad_fn=<MseLossBackward>)\n",
      "   263 / 1000 epoch mean error is  ------------   tensor(0.0988, grad_fn=<MseLossBackward>)\n",
      "   264 / 1000 epoch mean error is  ------------   tensor(0.1381, grad_fn=<MseLossBackward>)\n",
      "   265 / 1000 epoch mean error is  ------------   tensor(0.1749, grad_fn=<MseLossBackward>)\n",
      "   266 / 1000 epoch mean error is  ------------   tensor(0.1367, grad_fn=<MseLossBackward>)\n",
      "   267 / 1000 epoch mean error is  ------------   tensor(0.0986, grad_fn=<MseLossBackward>)\n",
      "   268 / 1000 epoch mean error is  ------------   tensor(0.1545, grad_fn=<MseLossBackward>)\n",
      "   269 / 1000 epoch mean error is  ------------   tensor(0.1163, grad_fn=<MseLossBackward>)\n",
      "   270 / 1000 epoch mean error is  ------------   tensor(0.1737, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   271 / 1000 epoch mean error is  ------------   tensor(0.1146, grad_fn=<MseLossBackward>)\n",
      "   272 / 1000 epoch mean error is  ------------   tensor(0.1820, grad_fn=<MseLossBackward>)\n",
      "   273 / 1000 epoch mean error is  ------------   tensor(0.1824, grad_fn=<MseLossBackward>)\n",
      "   274 / 1000 epoch mean error is  ------------   tensor(0.1390, grad_fn=<MseLossBackward>)\n",
      "   275 / 1000 epoch mean error is  ------------   tensor(0.2192, grad_fn=<MseLossBackward>)\n",
      "   276 / 1000 epoch mean error is  ------------   tensor(0.0960, grad_fn=<MseLossBackward>)\n",
      "   277 / 1000 epoch mean error is  ------------   tensor(0.2244, grad_fn=<MseLossBackward>)\n",
      "   278 / 1000 epoch mean error is  ------------   tensor(0.1694, grad_fn=<MseLossBackward>)\n",
      "   279 / 1000 epoch mean error is  ------------   tensor(0.1351, grad_fn=<MseLossBackward>)\n",
      "   280 / 1000 epoch mean error is  ------------   tensor(0.1910, grad_fn=<MseLossBackward>)\n",
      "   281 / 1000 epoch mean error is  ------------   tensor(0.1638, grad_fn=<MseLossBackward>)\n",
      "   282 / 1000 epoch mean error is  ------------   tensor(0.1785, grad_fn=<MseLossBackward>)\n",
      "   283 / 1000 epoch mean error is  ------------   tensor(0.2207, grad_fn=<MseLossBackward>)\n",
      "   284 / 1000 epoch mean error is  ------------   tensor(0.1965, grad_fn=<MseLossBackward>)\n",
      "   285 / 1000 epoch mean error is  ------------   tensor(0.2207, grad_fn=<MseLossBackward>)\n",
      "   286 / 1000 epoch mean error is  ------------   tensor(0.1527, grad_fn=<MseLossBackward>)\n",
      "   287 / 1000 epoch mean error is  ------------   tensor(0.1495, grad_fn=<MseLossBackward>)\n",
      "   288 / 1000 epoch mean error is  ------------   tensor(0.1674, grad_fn=<MseLossBackward>)\n",
      "   289 / 1000 epoch mean error is  ------------   tensor(0.2603, grad_fn=<MseLossBackward>)\n",
      "   290 / 1000 epoch mean error is  ------------   tensor(0.2073, grad_fn=<MseLossBackward>)\n",
      "   291 / 1000 epoch mean error is  ------------   tensor(0.1349, grad_fn=<MseLossBackward>)\n",
      "   292 / 1000 epoch mean error is  ------------   tensor(0.1350, grad_fn=<MseLossBackward>)\n",
      "   293 / 1000 epoch mean error is  ------------   tensor(0.2196, grad_fn=<MseLossBackward>)\n",
      "   294 / 1000 epoch mean error is  ------------   tensor(0.2243, grad_fn=<MseLossBackward>)\n",
      "   295 / 1000 epoch mean error is  ------------   tensor(0.1939, grad_fn=<MseLossBackward>)\n",
      "   296 / 1000 epoch mean error is  ------------   tensor(0.2232, grad_fn=<MseLossBackward>)\n",
      "   297 / 1000 epoch mean error is  ------------   tensor(0.2934, grad_fn=<MseLossBackward>)\n",
      "   298 / 1000 epoch mean error is  ------------   tensor(0.1995, grad_fn=<MseLossBackward>)\n",
      "   299 / 1000 epoch mean error is  ------------   tensor(0.2839, grad_fn=<MseLossBackward>)\n",
      "   300 / 1000 epoch mean error is  ------------   tensor(0.3625, grad_fn=<MseLossBackward>)\n",
      "   301 / 1000 epoch mean error is  ------------   tensor(0.2488, grad_fn=<MseLossBackward>)\n",
      "   302 / 1000 epoch mean error is  ------------   tensor(0.3103, grad_fn=<MseLossBackward>)\n",
      "   303 / 1000 epoch mean error is  ------------   tensor(0.2225, grad_fn=<MseLossBackward>)\n",
      "   304 / 1000 epoch mean error is  ------------   tensor(0.3761, grad_fn=<MseLossBackward>)\n",
      "   305 / 1000 epoch mean error is  ------------   tensor(0.2724, grad_fn=<MseLossBackward>)\n",
      "   306 / 1000 epoch mean error is  ------------   tensor(0.3459, grad_fn=<MseLossBackward>)\n",
      "   307 / 1000 epoch mean error is  ------------   tensor(0.2722, grad_fn=<MseLossBackward>)\n",
      "   308 / 1000 epoch mean error is  ------------   tensor(0.2671, grad_fn=<MseLossBackward>)\n",
      "   309 / 1000 epoch mean error is  ------------   tensor(0.2820, grad_fn=<MseLossBackward>)\n",
      "   310 / 1000 epoch mean error is  ------------   tensor(0.2092, grad_fn=<MseLossBackward>)\n",
      "   311 / 1000 epoch mean error is  ------------   tensor(0.2583, grad_fn=<MseLossBackward>)\n",
      "   312 / 1000 epoch mean error is  ------------   tensor(0.2584, grad_fn=<MseLossBackward>)\n",
      "   313 / 1000 epoch mean error is  ------------   tensor(0.2911, grad_fn=<MseLossBackward>)\n",
      "   314 / 1000 epoch mean error is  ------------   tensor(0.3443, grad_fn=<MseLossBackward>)\n",
      "   315 / 1000 epoch mean error is  ------------   tensor(0.1505, grad_fn=<MseLossBackward>)\n",
      "   316 / 1000 epoch mean error is  ------------   tensor(0.2839, grad_fn=<MseLossBackward>)\n",
      "   317 / 1000 epoch mean error is  ------------   tensor(0.2960, grad_fn=<MseLossBackward>)\n",
      "   318 / 1000 epoch mean error is  ------------   tensor(0.2717, grad_fn=<MseLossBackward>)\n",
      "   319 / 1000 epoch mean error is  ------------   tensor(0.1653, grad_fn=<MseLossBackward>)\n",
      "   320 / 1000 epoch mean error is  ------------   tensor(0.4304, grad_fn=<MseLossBackward>)\n",
      "   321 / 1000 epoch mean error is  ------------   tensor(0.1623, grad_fn=<MseLossBackward>)\n",
      "   322 / 1000 epoch mean error is  ------------   tensor(0.2709, grad_fn=<MseLossBackward>)\n",
      "   323 / 1000 epoch mean error is  ------------   tensor(0.3490, grad_fn=<MseLossBackward>)\n",
      "   324 / 1000 epoch mean error is  ------------   tensor(0.2527, grad_fn=<MseLossBackward>)\n",
      "   325 / 1000 epoch mean error is  ------------   tensor(0.3134, grad_fn=<MseLossBackward>)\n",
      "   326 / 1000 epoch mean error is  ------------   tensor(0.2462, grad_fn=<MseLossBackward>)\n",
      "   327 / 1000 epoch mean error is  ------------   tensor(0.2700, grad_fn=<MseLossBackward>)\n",
      "   328 / 1000 epoch mean error is  ------------   tensor(0.1004, grad_fn=<MseLossBackward>)\n",
      "   329 / 1000 epoch mean error is  ------------   tensor(0.2006, grad_fn=<MseLossBackward>)\n",
      "   330 / 1000 epoch mean error is  ------------   tensor(0.2454, grad_fn=<MseLossBackward>)\n",
      "   331 / 1000 epoch mean error is  ------------   tensor(0.3393, grad_fn=<MseLossBackward>)\n",
      "   332 / 1000 epoch mean error is  ------------   tensor(0.2256, grad_fn=<MseLossBackward>)\n",
      "   333 / 1000 epoch mean error is  ------------   tensor(0.2085, grad_fn=<MseLossBackward>)\n",
      "   334 / 1000 epoch mean error is  ------------   tensor(0.1837, grad_fn=<MseLossBackward>)\n",
      "   335 / 1000 epoch mean error is  ------------   tensor(0.1052, grad_fn=<MseLossBackward>)\n",
      "   336 / 1000 epoch mean error is  ------------   tensor(0.2962, grad_fn=<MseLossBackward>)\n",
      "   337 / 1000 epoch mean error is  ------------   tensor(0.1658, grad_fn=<MseLossBackward>)\n",
      "   338 / 1000 epoch mean error is  ------------   tensor(0.2003, grad_fn=<MseLossBackward>)\n",
      "   339 / 1000 epoch mean error is  ------------   tensor(0.1846, grad_fn=<MseLossBackward>)\n",
      "   340 / 1000 epoch mean error is  ------------   tensor(0.2264, grad_fn=<MseLossBackward>)\n",
      "   341 / 1000 epoch mean error is  ------------   tensor(0.1649, grad_fn=<MseLossBackward>)\n",
      "   342 / 1000 epoch mean error is  ------------   tensor(0.2094, grad_fn=<MseLossBackward>)\n",
      "   343 / 1000 epoch mean error is  ------------   tensor(0.2315, grad_fn=<MseLossBackward>)\n",
      "   344 / 1000 epoch mean error is  ------------   tensor(0.2379, grad_fn=<MseLossBackward>)\n",
      "   345 / 1000 epoch mean error is  ------------   tensor(0.1645, grad_fn=<MseLossBackward>)\n",
      "   346 / 1000 epoch mean error is  ------------   tensor(0.1280, grad_fn=<MseLossBackward>)\n",
      "   347 / 1000 epoch mean error is  ------------   tensor(0.1702, grad_fn=<MseLossBackward>)\n",
      "   348 / 1000 epoch mean error is  ------------   tensor(0.2669, grad_fn=<MseLossBackward>)\n",
      "   349 / 1000 epoch mean error is  ------------   tensor(0.2291, grad_fn=<MseLossBackward>)\n",
      "   350 / 1000 epoch mean error is  ------------   tensor(0.2467, grad_fn=<MseLossBackward>)\n",
      "   351 / 1000 epoch mean error is  ------------   tensor(0.1463, grad_fn=<MseLossBackward>)\n",
      "   352 / 1000 epoch mean error is  ------------   tensor(0.2460, grad_fn=<MseLossBackward>)\n",
      "   353 / 1000 epoch mean error is  ------------   tensor(0.3390, grad_fn=<MseLossBackward>)\n",
      "   354 / 1000 epoch mean error is  ------------   tensor(0.2026, grad_fn=<MseLossBackward>)\n",
      "   355 / 1000 epoch mean error is  ------------   tensor(0.1818, grad_fn=<MseLossBackward>)\n",
      "   356 / 1000 epoch mean error is  ------------   tensor(0.1934, grad_fn=<MseLossBackward>)\n",
      "   357 / 1000 epoch mean error is  ------------   tensor(0.1713, grad_fn=<MseLossBackward>)\n",
      "   358 / 1000 epoch mean error is  ------------   tensor(0.1531, grad_fn=<MseLossBackward>)\n",
      "   359 / 1000 epoch mean error is  ------------   tensor(0.2125, grad_fn=<MseLossBackward>)\n",
      "   360 / 1000 epoch mean error is  ------------   tensor(0.1521, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   361 / 1000 epoch mean error is  ------------   tensor(0.3149, grad_fn=<MseLossBackward>)\n",
      "   362 / 1000 epoch mean error is  ------------   tensor(0.2205, grad_fn=<MseLossBackward>)\n",
      "   363 / 1000 epoch mean error is  ------------   tensor(0.2612, grad_fn=<MseLossBackward>)\n",
      "   364 / 1000 epoch mean error is  ------------   tensor(0.2654, grad_fn=<MseLossBackward>)\n",
      "   365 / 1000 epoch mean error is  ------------   tensor(0.1869, grad_fn=<MseLossBackward>)\n",
      "   366 / 1000 epoch mean error is  ------------   tensor(0.2255, grad_fn=<MseLossBackward>)\n",
      "   367 / 1000 epoch mean error is  ------------   tensor(0.1993, grad_fn=<MseLossBackward>)\n",
      "   368 / 1000 epoch mean error is  ------------   tensor(0.1689, grad_fn=<MseLossBackward>)\n",
      "   369 / 1000 epoch mean error is  ------------   tensor(0.3380, grad_fn=<MseLossBackward>)\n",
      "   370 / 1000 epoch mean error is  ------------   tensor(0.2243, grad_fn=<MseLossBackward>)\n",
      "   371 / 1000 epoch mean error is  ------------   tensor(0.2754, grad_fn=<MseLossBackward>)\n",
      "   372 / 1000 epoch mean error is  ------------   tensor(0.2436, grad_fn=<MseLossBackward>)\n",
      "   373 / 1000 epoch mean error is  ------------   tensor(0.1411, grad_fn=<MseLossBackward>)\n",
      "   374 / 1000 epoch mean error is  ------------   tensor(0.1264, grad_fn=<MseLossBackward>)\n",
      "   375 / 1000 epoch mean error is  ------------   tensor(0.1602, grad_fn=<MseLossBackward>)\n",
      "   376 / 1000 epoch mean error is  ------------   tensor(0.2289, grad_fn=<MseLossBackward>)\n",
      "   377 / 1000 epoch mean error is  ------------   tensor(0.2115, grad_fn=<MseLossBackward>)\n",
      "   378 / 1000 epoch mean error is  ------------   tensor(0.0664, grad_fn=<MseLossBackward>)\n",
      "   379 / 1000 epoch mean error is  ------------   tensor(0.0663, grad_fn=<MseLossBackward>)\n",
      "   380 / 1000 epoch mean error is  ------------   tensor(0.1511, grad_fn=<MseLossBackward>)\n",
      "   381 / 1000 epoch mean error is  ------------   tensor(0.3142, grad_fn=<MseLossBackward>)\n",
      "   382 / 1000 epoch mean error is  ------------   tensor(0.1834, grad_fn=<MseLossBackward>)\n",
      "   383 / 1000 epoch mean error is  ------------   tensor(0.1799, grad_fn=<MseLossBackward>)\n",
      "   384 / 1000 epoch mean error is  ------------   tensor(0.2249, grad_fn=<MseLossBackward>)\n",
      "   385 / 1000 epoch mean error is  ------------   tensor(0.3254, grad_fn=<MseLossBackward>)\n",
      "   386 / 1000 epoch mean error is  ------------   tensor(0.1229, grad_fn=<MseLossBackward>)\n",
      "   387 / 1000 epoch mean error is  ------------   tensor(0.2101, grad_fn=<MseLossBackward>)\n",
      "   388 / 1000 epoch mean error is  ------------   tensor(0.2334, grad_fn=<MseLossBackward>)\n",
      "   389 / 1000 epoch mean error is  ------------   tensor(0.2199, grad_fn=<MseLossBackward>)\n",
      "   390 / 1000 epoch mean error is  ------------   tensor(0.1454, grad_fn=<MseLossBackward>)\n",
      "   391 / 1000 epoch mean error is  ------------   tensor(0.1639, grad_fn=<MseLossBackward>)\n",
      "   392 / 1000 epoch mean error is  ------------   tensor(0.1211, grad_fn=<MseLossBackward>)\n",
      "   393 / 1000 epoch mean error is  ------------   tensor(0.1083, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training:\n",
    "\n",
    "From the readout onset the network is unrolled \n",
    "\n",
    "and the backpropagation goes back in time until the beginning of the readout.\n",
    "\n",
    "Backprop is called after computing the error on the whole batch.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "mse_loss = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
    "losses_record = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "\n",
    "        \n",
    "        x = [None]*(read_onset+1)\n",
    "        x[0] = torch.zeros([trials, model.N, 1])\n",
    "        u, y = generate_data(T, trials, stim_strength)\n",
    "        \n",
    "        for t in range(T-read_onset):\n",
    "            \n",
    "            # Override x zero.\n",
    "            x[0] = model.lr_rnn.forward(x[0], u[t], tau, dt)\n",
    "            \n",
    "        x[0].detach_()\n",
    " \n",
    "        # Option 0\n",
    "        z = torch.zeros(read_onset, trials)       \n",
    "        \n",
    "        for k in range(read_onset):\n",
    "\n",
    "            # Initialization.\n",
    "            t = T - read_onset + k        \n",
    "            \n",
    "            # Computation of the hidden layer and the readout, roll-out.                \n",
    "            x[k+1], z[k] = model.forward(x[k], u[t], tau, dt)\n",
    "\n",
    "\n",
    "            # Option 1\n",
    "            # use the error or mse.\n",
    "            #loss[k] = mse_loss(z, y.float())\n",
    "            \n",
    "            \n",
    "            \n",
    "        optimizer.zero_grad()  # Reset the gradient values.\n",
    "\n",
    "        #losses = torch.sum(loss) / read_onset\n",
    "        losses = mse_loss(z, y.float())\n",
    "        \n",
    "        losses.backward()  # Compute the gradients.\n",
    "\n",
    "        optimizer.step()  # Parameters update.\n",
    "            \n",
    "        #if epoch%10 == 0:\n",
    "        \n",
    "        print(\"  \", epoch,\"/\",epochs, \"epoch mean error is  ------------  \", losses)\n",
    "        \n",
    "        losses_record.append(losses.detach())\n",
    "        losses.detach()\n",
    "        z.detach()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why $x$ should be set to zero after each trial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, torch.sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in model.parameters():\n",
    "    print(parameter.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a_trials = 2\n",
    "a_model = Model(a_trials, 512)\n",
    "a_T = 74\n",
    "a_u, a_y = generate_data(a_T, a_trials, stim_strength)\n",
    "a_x = torch.zeros([a_trials,512,1])\n",
    "\n",
    "a_tau = 0.1\n",
    "a_dt = 0.02\n",
    "a_x, a_z = a_model(a_x, a_u[50], a_tau, a_dt)\n",
    "a_u.shape, a_x.shape, a_z.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
